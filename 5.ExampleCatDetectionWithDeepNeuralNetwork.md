# Example: Cat Detection with Deep Neural Network

What should we do for deep neural network is very similar to that for shallow neural network which we built in the previous example.

# Differences from Shallow Neural Network

1. **Initializing weights and biases:** A deep NN has more than two weights and biases so we should initialize them according to how many layers the network has. 
2. **Forward propagation:** Every layer does two operations that are linear computation ($Z=WX+B$) and activation computation ($A=g(Z)$). Just repeat these operations for each layer.
3. **Backward propagation:** Similar to forward propagation, backward propagation also does same operations that produce $dW^{[l]}$ and $db^{[l]}$ needed in gradient descent. Note that the output layer uses `sigmoid` as activation function instead of `relu` that is used by hidden layers.
4. **Updating parameters:** We need to use a loop to update all $W^{[l]}$ and $b^{[l]}$.

# Architecture of NN

In code implementation, the program should be compatible with different networks regardless of how many layers or how many neurons (units) they have. Because we cannot decide which architecture we should choose without any tries.

In this example, we choose the following architecture as our initial choice.

```python
def get_layer_sizes():
    # A five layer neural network

    n_x = train_x.shape[0]
    n_h1 = 20
    n_h2 = 10
    n_h3 = 5
    n_h4 = 3
    n_y = train_y.shape[0]

    return n_x, n_h1, n_h2, n_h3, n_h4, n_y
```

This function can returen a tuple with unsure length, so that we can calculate the number of layers $L$ through `L=len(get_layer_sizes())-1` (Attention that the input layer is ignored when calculating $L$).

# Full Implementation in Python

```python
def relu(x):
    return np.where(x > 0, x, 0)


def derivative_relu(x):
    return np.where(x > 0, 1, 0)


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def initialize_parameters(layer_sizes):
    parameters = {}
    L = len(layer_sizes) - 1

    for l in range(1, L + 1):
      	# Using a dictionary is one of ways to store multiple layers' parameters or caches.
        
        parameters[f"W{l}"] = np.random.randn(layer_sizes[l], layer_sizes[l - 1]) / np.sqrt(layer_sizes[l - 1]) # Divide the square root of the matrix's column size to shrink the value of matrix mulplication.
        parameters[f"b{l}"] = np.zeros((layer_sizes[l], 1))

    return parameters


def forward_propagation(parameters, x):
    L = len(parameters) // 2 # `parameters` is [W1, b1, W2, b2,...].

    forward_caches = {
        "A0": x
    } # Use this dictionary to store As and Zs computed in forward propagation.

    for l in range(1, L):
        forward_caches[f"Z{l}"] = np.dot(parameters[f"W{l}"], forward_caches[f"A{l - 1}"]) + parameters[f"b{l}"]
        forward_caches[f"A{l}"] = relu(forward_caches[f"Z{l}"])

    forward_caches[f"Z{L}"] = np.dot(parameters[f"W{L}"], forward_caches[f"A{L - 1}"]) + parameters[f"b{L}"]
    forward_caches[f"A{L}"] = sigmoid(forward_caches[f"Z{L}"]) # The output layer uses sigmoid.

    return forward_caches


def backward_propagation(parameters, forward_caches, y):
    gradients = {} # Store dWs and dbs
    backward_caches = {} # Store dZs and dA, which are junks when backward propagation finished.
    L = len(parameters) // 2
    m = y.shape[1]

    backward_caches[f"dZ{L}"] = forward_caches[f"A{L}"] - y # Derivative of (dJ/dA)*(dA/dZ), where `J` is cost function and `A` is sigmoid.
    gradients[f"dW{L}"] = np.dot(backward_caches[f"dZ{L}"], forward_caches[f"A{L - 1}"].T) / m
    gradients[f"db{L}"] = np.sum(backward_caches[f"dZ{L}"], axis=1, keepdims=True) / m
    backward_caches[f"dA{L - 1}"] = np.dot(parameters[f"W{L}"].T, backward_caches[f"dZ{L}"])

    for l in reversed(range(1, L)):
      	# We need to compute derivative values backward.
        
        backward_caches[f"dZ{l}"] = np.multiply(backward_caches[f"dA{l}"], derivative_relu(forward_caches[f"Z{l}"]))
        gradients[f"dW{l}"] = np.dot(backward_caches[f"dZ{l}"], forward_caches[f"A{l - 1}"].T) / m
        gradients[f"db{l}"] = np.sum(backward_caches[f"dZ{l}"], axis=1, keepdims=True) / m
        backward_caches[f"dA{l - 1}"] = np.dot(parameters[f"W{l}"].T, backward_caches[f"dZ{l}"])

    return gradients


def compute_cost(AL, y):
  	# `AL` is the result of the neural network, which can be marked as "y hat" in math formulas.
    return np.squeeze(-np.sum(
        (np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL)))) / m_train)


def update_parameters(parameters, gradients, learning_rate=0.0075):
    L = len(parameters) // 2

    for l in range(1, L + 1):
        parameters[f"W{l}"] -= learning_rate * gradients[f"dW{l}"]
        parameters[f"b{l}"] -= learning_rate * gradients[f"db{l}"]

    return parameters


def nn_model(num_iter, print_cost=False, is_plot=False):
    layer_sizes = get_layer_sizes()
    parameters = initialize_parameters(layer_sizes)
    L = len(parameters) // 2
    costs = []

    for i in range(num_iter):
        forward_caches = forward_propagation(parameters, train_x)
        cost = compute_cost(forward_caches[f"A{L}"], train_y)
        gradients = backward_propagation(parameters, forward_caches, train_y)
        parameters = update_parameters(parameters, gradients)

        if print_cost and i % 100 == 0:
            costs.append(cost)
            print("Cost after iteration %i: %f" % (i, cost))

    if is_plot:
        plt.plot(np.squeeze(costs))
        plt.ylabel('Cost')
        plt.xlabel('Iterations')
        plt.title("Cost Change")
        plt.show()

    return parameters


def predict(parameters, x):
    L = len(parameters) // 2
    forward_caches = forward_propagation(parameters, x)
    return forward_caches[f"A{L}"]
```



Additionally, we can use following code to evaluate our model:

```python
parameters = nn_model(2500, print_cost=True, is_plot=True)

Y_hat = predict(parameters, test_x)
# Classify cat and non-cat according to whether AL is greater than 0.5 or not.
Y_hat[Y_hat < 0.5] = 0
Y_hat[Y_hat >= 0.5] = 1
accuracies = Y_hat == test_set_y_orig.reshape(1, -1)
accuracy_rate = np.sum(accuracies) / m_test
print(f"Accuracy: {accuracy_rate * 100 : .2f}%")

while True:
    index = int(input("Test No: "))
    x = test_x[:, index].reshape(-1, 1)
    y = np.squeeze(predict(parameters, x))
    plt.imshow(test_set_x_orig[index])
    plt.title(f"There is {y * 100 : .2f}% chances that this is a cat.")
    plt.show()
```

