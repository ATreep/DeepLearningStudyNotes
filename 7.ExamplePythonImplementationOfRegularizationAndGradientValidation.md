# Example: Python Implementation of Regularization & Gradient Validation

This example is based on the last cat detection project with adding L2 regularization, dropout, and gradient validation codes.

# L2 Regularization in Python

Let's update `compute_cost` function first.

What we need to do is to add a regularization component to cost function: $\frac{\lambda}{2m}\sum_{l=1}^{L}\parallel w^{[l]}\parallel ^{2}_F$

We can use `np.square(np.linalg.norm(w))` or `np.sum(np.square(w))` to calcute the Frobenius norm.

Here is our new `compute_cost` function:

```python
def compute_cost(AL, y, L, parameters, lambd=0):
    cost = np.squeeze(-np.sum(
        (np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL)))) / m_train)
    l2_com = 0
    for l in range(1, L + 1):
        l2_com += np.sum(np.square(parameters[f"W{l}"]))
    l2_com = l2_com * lambd / (2 * m_train)
    return cost + l2_com
```

Do not forget to update backward propagation as the cost function changed.

Change

```python
gradients[f"dW{l}"] = (np.dot(backward_caches[f"dZ{l}"], forward_caches[f"A{l - 1}"].T)) / m_train
```

into

```python
gradients[f"dW{l}"] = (np.dot(backward_caches[f"dZ{l}"], forward_caches[f"A{l - 1}"].T) + np.multiply(lambd, parameters[f"W{l}"])) / m_train
```

# Dropout in Python

Both forward propagation and backward propagation need to be updated.

Assuming that we only want to use dropout on the first hidden layer, we need to create dropout mask matrix $D1$ first which is the matrix including only 1 and 0 to choose which neurons can be reversed.

After that, what we do next is multiplication and division operations.

There is our code for dropout in **forward propagation**:

```python
forward_caches["D1"] = np.random.rand(*forward_caches["A1"].shape) <= keep_prob  # All elements bigger than keep-prob become 0, and others become 1. 
forward_caches["A1"] = np.multiply(forward_caches["A1"], forward_caches["D1"])  # Multiply A1 by D1 to eliminate some neurons into 0.
forward_caches["A1"] /= keep_prob
```

Similarly, here is what should be added in **backward propagation**:

```python
backward_caches["dA1"] = np.multiply(backward_caches["dA1"], forward_caches["D1"]) / keep_prob
```

# Gradient Validation

There are multiple operations that convert the `parameters` (`gradients`) dictionary into vector `theta` (`d_theta`) and some inverted operations, so recommend to implement the two operations with two functions.

Function `parameters_to_theta`:

```python
def parameters_to_theta(parameters, is_derivative):
  	# If you'd like to convert `gradients` dictionary (dW, db) into d_theta, set True to `is_derivative`.
    
    d = "d" if is_derivative else ""  # `gradients`'s keys start with "d", such as gradients["dW1"].
    L = len(parameters) // 2  # Every layer have two parameters "W" and "b", so, half of its length is the size of L.
    shapes = []  # To recover the `parameters` dictionary from theta vector, record every parameter's shape (becuase theta is a very big vector that all parameters reshaped as (n, 1) and combined together, and you cannot distinguish them directly).

    W1 = parameters[d+"W1"].reshape(-1, 1)
    theta = W1
    shapes.append(parameters[d+"W1"].shape)

    b1 = parameters[d+"b1"].reshape(-1, 1)
    theta = np.concatenate((theta, b1))  # Use np.concatenate to combine all reshaped matrix into a big vector.
    shapes.append(parameters[d+"b1"].shape)

    if L > 1:
        for l in range(2, L + 1):
          	# Concatenate other matrices
            
            W = parameters[d+f"W{l}"].reshape(-1, 1)
            theta = np.concatenate((theta, W))
            shapes.append(parameters[d+f"W{l}"].shape)

            b = parameters[d+f"b{l}"].reshape(-1, 1)
            theta = np.concatenate((theta, b))
            shapes.append(parameters[d+f"b{l}"].shape)

    return theta, shapes
```



Create its inverted function:

```python
def theta_to_parameters(theta, shapes):
  	# We do not need to convert d_theta to `gradients` dictionary.
    
    parameters = {}
    L = len(shapes) // 2  # The length of list `shapes` is the number of all parameters.
    i = 0  # `i` is an index to indicate the current operating position of theta.
    for l in range(1, L + 1):
      	# `l` is the index of layers started from 1, `(l - 1) * 2` is the index of `W[l]` in the list and `(l - 1) * 2 + 1` is the index of `b[l]` in the list.
        # For a matrix, its (shape[0] * shape[1]) is the size of its flattened vector.
        
        parameters[f"W{l}"] = (theta[i:i + (shapes[(l - 1) * 2][0] * shapes[(l - 1) * 2][1])]
                               .reshape(*shapes[(l - 1) * 2]))
        i += (shapes[(l - 1) * 2][0] * shapes[(l - 1) * 2][1])  # `i` jumps to the index of the beginning element of the next parameter.
        parameters[f"b{l}"] = (theta[i:i + (shapes[(l - 1) * 2 + 1][0] * shapes[(l - 1) * 2 + 1][1])]
                               .reshape(*shapes[(l - 1) * 2 + 1]))
        i += (shapes[(l - 1) * 2 + 1][0] * shapes[(l - 1) * 2 + 1][1])

    return parameters
```



In gradient validation, each element $\theta_i$ of $\theta$ has an operation that compute the slope of $J$ in $U(\theta_i,\epsilon)$, i.e. a section centered at $\theta_i$, and length $2\epsilon$, which is the operation to compute the approximate derivative of theta.

Combine parameters into theta -> Compute the approximate derivative of theta -> Compute the difference

```python
def gradient_validate(parameters, gradients, lambd=0):
    L = len(parameters) // 2
    theta, theta_sizes = parameters_to_theta(parameters, False)
    d_theta, d_theta_sizes = parameters_to_theta(gradients, True)
    d_theta_approx = np.zeros(d_theta.shape)
    epsilon = 1e-7
    for i in range(theta.shape[0]):
      	# Compute the slope for every element.
        
        theta_plus = np.copy(theta)
        theta_plus[i] += epsilon  # Do not change other parameters of J, because we are calculating the partial derivative of J with respect to theta[i].
        theta_minus = np.copy(theta)
        theta_minus[i] -= epsilon

        parameters_plus = theta_to_parameters(theta_plus, theta_sizes)  # To compute J(..., theta_i + epsilon, ...).
        cost_plus = compute_cost(forward_propagation(parameters_plus, train_x, keep_prob=1)[f"A{L}"], train_set_y_orig, L, parameters_plus, lambd)

        parameters_minus = theta_to_parameters(theta_minus, theta_sizes)
        cost_minus = compute_cost(forward_propagation(parameters_minus, train_x, keep_prob=1)[f"A{L}"], train_set_y_orig, L, parameters_minus, lambd)

        d_theta_approx[i] = (cost_plus - cost_minus) / (2 * epsilon)  # Compute the slope of J.

    difference = np.linalg.norm(d_theta - d_theta_approx) / (np.linalg.norm(d_theta) + np.linalg.norm(d_theta_approx))  # Compute the difference between the approximate derivative and the derivative computed by backward propagation.
    return difference
```

If difference is greater than $\epsilon$ or $10^{-7}$, your derivatives may be wrong in backward propagation.